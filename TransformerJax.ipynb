{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1hDfjdUMaOO00WyflWO-VCSIQnRvpAOUh",
      "authorship_tag": "ABX9TyNmI0tKxgU8Os7NjnYPC+pi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhenyiqi/rawLLM/blob/main/TransformerJax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "Q611jPYD5jFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "from functools import partial"
      ],
      "metadata": {
        "id": "b2QUL3oX5-1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPShDzNzouNd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from torch.utils import data\n",
        "from torchvision.datasets import MNIST"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "from jax.scipy.special import logsumexp\n",
        "\n",
        "from jax import grad, jit, vmap\n",
        "from jax import random\n",
        "import jax\n",
        "from flax.training import train_state, checkpoints"
      ],
      "metadata": {
        "id": "JIYsEBneoyKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install flax --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBf1OGtN9emw",
        "outputId": "724796ea-b562-454d-fa00-dfcc53fb8fd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |█▋                              | 10 kB 20.2 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 20 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |█████                           | 30 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 40 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 51 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 61 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 81 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 92 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 102 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 112 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 122 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 133 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 143 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 153 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 163 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 174 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 184 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 194 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 197 kB 4.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 154 kB 50.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 66 kB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 238 kB 61.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 8.3 MB 68.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 51 kB 7.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 85 kB 3.4 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import flax\n",
        "from flax import linen as nn"
      ],
      "metadata": {
        "id": "xrtXGOKb8Org"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Optax (Optimizers in JAX)\n",
        "try:\n",
        "    import optax\n",
        "except ModuleNotFoundError: # Install optax if missing\n",
        "    !pip install --quiet optax\n",
        "    import optax"
      ],
      "metadata": {
        "id": "_VNe0GjbxH9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define constants"
      ],
      "metadata": {
        "id": "gxqXzVOC5hLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rng_main = random.PRNGKey(0)"
      ],
      "metadata": {
        "id": "Wll64TI0uZzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "n_targets = 10\n",
        "num_epochs = 5\n",
        "\n",
        "layer_sizes = [784, 512, 512, 10]\n",
        "step_size = 0.01"
      ],
      "metadata": {
        "id": "JbnG56wtr3Bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Prep"
      ],
      "metadata": {
        "id": "Oreq5dxN5Zd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rng = random.PRNGKey(1)"
      ],
      "metadata": {
        "id": "P_QThnW_82No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils for initalizing parameters"
      ],
      "metadata": {
        "id": "qr0cDWZjFi9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from traitlets.traitlets import Tuple\n",
        "\n",
        "# A helper function to randomly initialize weights and biases\n",
        "# for a dense neural network layer\n",
        "def random_layer_params(input_dim, output_dim, key, scale=1e-2):\n",
        "  w_key, b_key = random.split(key)\n",
        "  # random.normal(w_key, (n, m)) generates a random matrix of dimension (n, m)\n",
        "  return (scale * random.normal(w_key, (output_dim, input_dim)),\n",
        "          scale * random.normal(b_key, (output_dim,)))\n",
        "\n",
        "# Initialize all layers for a fully-connected neural network with sizes \"sizes\"\n",
        "def init_network_params(sizes, key: ...):\n",
        "  keys = random.split(key, len(sizes))\n",
        "  return [random_layer_params(m, n, k)\n",
        "          for m, n, k in zip(sizes[:-1], sizes[1:], keys)]"
      ],
      "metadata": {
        "id": "pu2jrX2z6gQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Core training utils"
      ],
      "metadata": {
        "id": "TmluYmusFnxE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Builds the model"
      ],
      "metadata": {
        "id": "fyms2hYhJJrg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Scaled-dot production attention"
      ],
      "metadata": {
        "id": "WtrE1KxW_xud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask = None):\n",
        "  \"\"\"Q attend to the K-V pair.\n",
        "\n",
        "  q, k, v are matrices of size (batch_size * ) T * k_q, where, T is the sequence length,\n",
        "  and k_q is the hidden dimension.\n",
        "  We may add dimensions in the front for parallelized computation.\"\"\"\n",
        "  # print('query is: ', q)\n",
        "  # print('key is: ', k)\n",
        "  # print('value is: ', v)\n",
        "  k_q = q.shape[-1]\n",
        "\n",
        "  # attention_logits is of dimension (batch_size * ) T * T\n",
        "  attention_logits = jnp.matmul(q, jnp.swapaxes(k, -1, -2)) / math.sqrt(k_q)\n",
        "  if mask is not None:\n",
        "    attention_logits = jnp.where(mask == 0, -9e15, attention_logits)\n",
        "  attention = nn.softmax(attention_logits)\n",
        "\n",
        "  # values is of dimension (batch_size * ) T * k_q again.\n",
        "  values = jnp.matmul(attention, v)\n",
        "  return values"
      ],
      "metadata": {
        "id": "EWdSeMbyjTzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MLP Layer"
      ],
      "metadata": {
        "id": "znJ9sOulu9C9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "  return jnp.maximum(0, x)\n",
        "\n",
        "class MLPLayer(nn.Module):\n",
        "  hidden_dim: int\n",
        "  num_hidden: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.layer_norm = nn.LayerNorm()\n",
        "    self.dense_layers = [nn.Dense(\n",
        "        self.hidden_dim) for _ in range(self.num_hidden)]\n",
        "\n",
        "  def __call__(self, input):\n",
        "    x = input\n",
        "    for layer in self.dense_layers:\n",
        "      x = layer(x)\n",
        "    x = self.layer_norm(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "hRCXuxAVvAQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MultiHeadAttention Layer"
      ],
      "metadata": {
        "id": "WIsPySahvESk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttnLayer(nn.Module):\n",
        "  output_dim: int  # output_dim == embedding_dim\n",
        "  num_heads: int = 1\n",
        "\n",
        "  def setup(self):\n",
        "    if self.output_dim % (self.num_heads) != 0:\n",
        "      raise ValueError(\n",
        "          'output_dim for a MultiHeadAttnLayer must be multiples of '\n",
        "          'num_heads.')\n",
        "\n",
        "    # Projection layers:\n",
        "    # 1) [0, num_heads) are applied to q\n",
        "    # 2) [num_heads, num_heads * 2) are applied to k\n",
        "    # 3) [2 *num_heads, 3 * num_heads) are applied to v\n",
        "    self.qkv_projs = [nn.Dense(self.output_dim // self.num_heads,\n",
        "                               kernel_init=nn.initializers.xavier_uniform(),  # Weights with Xavier uniform init\n",
        "                               bias_init=nn.initializers.zeros) for _ in range(3 * self.num_heads)]\n",
        "\n",
        "    self.layer_norm = nn.LayerNorm()\n",
        "\n",
        "\n",
        "  def __call__(self, q, k, v, mask = None):\n",
        "    batch_size, sequence_length, embed_dim = q.shape\n",
        "\n",
        "    # after the code block below, the dimension of Q, K, V becomes\n",
        "    # [batch_size, sequence_length, output_dim / (3 * num_heads), 3]\n",
        "    q = jnp.stack(tuple(self.qkv_projs[i](q) for i in range(self.num_heads)))\n",
        "    k = jnp.stack(tuple(self.qkv_projs[i](k) for i in range(self.num_heads, 2 * self.num_heads)))\n",
        "    v = jnp.stack(tuple(self.qkv_projs[i](k) for i in range(2 * self.num_heads, 3 * self.num_heads)))\n",
        "\n",
        "    q = q.reshape(batch_size, sequence_length, -1)\n",
        "    k = k.reshape(batch_size, sequence_length, -1)\n",
        "    v = v.reshape(batch_size, sequence_length, -1)\n",
        "\n",
        "    values = scaled_dot_product_attention(q, k, v, mask=mask)\n",
        "    # skip-add operation\n",
        "    values = values + q\n",
        "    values = self.layer_norm(values)\n",
        "\n",
        "    return values"
      ],
      "metadata": {
        "id": "K6RaTt0BvJo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Encoder Block"
      ],
      "metadata": {
        "id": "rUsvyE79vLYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "  input_dim: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.multi_head_attn_layer = MultiHeadAttnLayer(output_dim=self.input_dim)\n",
        "    self.mlp_layer = MLPLayer(hidden_dim=self.input_dim, num_hidden=5)\n",
        "\n",
        "  def __call__(self, q, k, v):\n",
        "    values = self.multi_head_attn_layer(q, k, v)\n",
        "    output = self.mlp_layer(values)\n",
        "    return output"
      ],
      "metadata": {
        "id": "xRB2OSTwvN0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Encoder"
      ],
      "metadata": {
        "id": "ilR5tQPyvO7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  num_encoder_block: int\n",
        "  input_dim: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.encoder_block = EncoderBlock(input_dim=self.input_dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    for i in range(self.num_encoder_block):\n",
        "      x = self.encoder_block(x, x, x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "5zuVB5IDvQsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decoder Block"
      ],
      "metadata": {
        "id": "BdS8txoYvSI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "  input_dim: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.multi_head_attn_layer = MultiHeadAttnLayer(output_dim=self.input_dim)\n",
        "    self.cross_attn_layer = MultiHeadAttnLayer(output_dim=self.input_dim)\n",
        "    self.mlp_layer = MLPLayer(hidden_dim=self.input_dim, num_hidden=5)\n",
        "\n",
        "  def __call__(self, y, x):\n",
        "    \"\"\"y is either the initial input of decoder or output of last DecoderBlock.\n",
        "    x is the output from the Encoder.\"\"\"\n",
        "    q = self.multi_head_attn_layer(y, y, y)\n",
        "    k, v = x, x\n",
        "    y = self.cross_attn_layer(q, k, v)\n",
        "    y = self.mlp_layer(y)\n",
        "    return y"
      ],
      "metadata": {
        "id": "yYHmPTuivUCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decoder"
      ],
      "metadata": {
        "id": "K_MVZ1XJvWuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  num_decoder_block: int\n",
        "  input_dim: int\n",
        "  output_dim: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.decoder_block = DecoderBlock(input_dim=self.input_dim)\n",
        "    self.projection = nn.Dense(features=self.output_dim)\n",
        "\n",
        "  def __call__(self, x, y, is_train: bool = False):\n",
        "    for i in range(self.num_decoder_block):\n",
        "      if not is_train:\n",
        "        y = self.decoder_block(y, x)\n",
        "\n",
        "    return y"
      ],
      "metadata": {
        "id": "ds3KjIRzvYTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Transformer"
      ],
      "metadata": {
        "id": "94rD6oKHvZqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  num_encoder_block: int\n",
        "  num_decoder_block: int\n",
        "  # input_dim is not sequence length, it's the embedding dimension\n",
        "  input_dim: int\n",
        "  output_dim: int\n",
        "  def setup(self):\n",
        "    self.encoder = Encoder(num_encoder_block=self.num_encoder_block,\n",
        "                           input_dim=self.input_dim)\n",
        "    self.decoder = Decoder(num_decoder_block=self.num_decoder_block,\n",
        "                           input_dim=self.input_dim,\n",
        "                           output_dim=self.output_dim)\n",
        "\n",
        "  def __call__(self, x, y):\n",
        "    x = self.encoder(x)\n",
        "    output = self.decoder(x, y)\n",
        "    return output"
      ],
      "metadata": {
        "id": "GRORNEdQJI8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize Transformer"
      ],
      "metadata": {
        "id": "H4864QROqmhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Transformer(num_encoder_block=2,\n",
        "                          num_decoder_block=3,\n",
        "                          input_dim=10,\n",
        "                          output_dim=10)"
      ],
      "metadata": {
        "id": "lemOR6C975uI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create a dummy example as the input to initialize the linen module.\n"
      ],
      "metadata": {
        "id": "jeAY0aMyuQk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rng_params, rng_x, rng_y = random.split(rng_main, 3)"
      ],
      "metadata": {
        "id": "lPRSEmKdCxjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_x = random.normal(rng_x, (64, 20, 10))\n",
        "example_y = random.normal(rng_y, (64, 20, 10))"
      ],
      "metadata": {
        "id": "lBslqalgYYCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initialize the linen module"
      ],
      "metadata": {
        "id": "v18giwiqC8vb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = transformer.init(random.PRNGKey(0), example_x, example_y)['params']"
      ],
      "metadata": {
        "id": "E7lyZVZU8Dn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Try applying the model on the dummy input and see the output format"
      ],
      "metadata": {
        "id": "d8fWM60TDAIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out = transformer.apply({'params': params}, example_x, example_y)\n",
        "print('Out', out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTuKv1sttqAe",
        "outputId": "5058ecd9-ba75-4b0c-a9d6-9980696e837e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Out (64, 20, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wfZda_8t-jh",
        "outputId": "5c67c9f0-0156-404f-f036-350d2e5b200c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[[-0.81314814,  1.1046953 ,  0.4776487 , ...,  1.2827947 ,\n",
              "               -0.8177259 ,  0.68436754],\n",
              "              [-0.9457575 ,  1.0223197 ,  0.44467473, ...,  1.2053069 ,\n",
              "               -0.87547475,  0.7679628 ],\n",
              "              [ 0.8291353 , -1.0935264 , -0.3079091 , ..., -1.2200329 ,\n",
              "                0.7818424 , -0.8695053 ],\n",
              "              ...,\n",
              "              [-0.94948304,  1.0248268 ,  0.45207748, ...,  1.207757  ,\n",
              "               -0.868755  ,  0.7732253 ],\n",
              "              [ 0.72673786, -1.1195886 , -0.19017659, ..., -1.2199332 ,\n",
              "                0.7187557 , -0.9032288 ],\n",
              "              [-0.9352995 ,  1.0279328 ,  0.44403443, ...,  1.2102603 ,\n",
              "               -0.87217087,  0.7617321 ]],\n",
              "\n",
              "             [[-0.63446015,  1.1582543 ,  0.1800934 , ...,  1.2679591 ,\n",
              "               -0.7362134 ,  0.84023964],\n",
              "              [ 0.76549387, -1.1043112 , -0.28653726, ..., -1.2490824 ,\n",
              "                0.7787767 , -0.8253995 ],\n",
              "              [-0.5244601 ,  1.173768  ,  0.05919825, ...,  1.260003  ,\n",
              "               -0.70110697,  0.8402413 ],\n",
              "              ...,\n",
              "              [-0.5943236 ,  1.164451  ,  0.13514261, ...,  1.2644212 ,\n",
              "               -0.7364697 ,  0.8345025 ],\n",
              "              [-0.46722734,  1.1766708 , -0.03807398, ...,  1.2341907 ,\n",
              "               -0.6655057 ,  0.87532103],\n",
              "              [-1.1194671 ,  0.9863867 ,  0.6058963 , ...,  1.2141169 ,\n",
              "               -0.88737863,  0.8742763 ]],\n",
              "\n",
              "             [[ 0.6303829 , -1.1702802 , -0.17349073, ..., -1.2661736 ,\n",
              "                0.6609434 , -0.88924336],\n",
              "              [ 0.67692804, -1.1559732 , -0.19193281, ..., -1.2533475 ,\n",
              "                0.66945153, -0.91397405],\n",
              "              [ 0.70763206, -1.1560674 , -0.2756779 , ..., -1.2750483 ,\n",
              "                0.719731  , -0.8506348 ],\n",
              "              ...,\n",
              "              [-0.9028443 ,  1.044466  ,  0.417493  , ...,  1.2237705 ,\n",
              "               -0.8559852 ,  0.77337176],\n",
              "              [-0.8659889 ,  1.0475663 ,  0.3960921 , ...,  1.2278892 ,\n",
              "               -0.86807036,  0.73629713],\n",
              "              [-0.8832729 ,  1.0409284 ,  0.39708826, ...,  1.2213017 ,\n",
              "               -0.8686504 ,  0.75241935]],\n",
              "\n",
              "             ...,\n",
              "\n",
              "             [[ 0.8144755 , -1.0860397 , -0.3082708 , ..., -1.2290646 ,\n",
              "                0.7494857 , -0.856706  ],\n",
              "              [ 0.8354527 , -1.1035005 , -0.3942834 , ..., -1.2489858 ,\n",
              "                0.7857519 , -0.7998355 ],\n",
              "              [ 0.8335105 , -1.0889639 , -0.34801683, ..., -1.2360317 ,\n",
              "                0.7464802 , -0.85050035],\n",
              "              ...,\n",
              "              [-0.9742213 ,  0.99323946,  0.30015835, ...,  1.1165289 ,\n",
              "               -0.7540555 ,  0.94312984],\n",
              "              [ 0.8419894 , -1.0556585 , -0.28980988, ..., -1.2018135 ,\n",
              "                0.7612512 , -0.86465514],\n",
              "              [-0.909755  ,  0.96292317,  0.07300005, ...,  1.030578  ,\n",
              "               -0.6166704 ,  1.0975006 ]],\n",
              "\n",
              "             [[ 0.6820896 , -1.142348  , -0.21523307, ..., -1.2361375 ,\n",
              "                0.85975784, -0.7852618 ],\n",
              "              [-0.78740555,  1.085316  ,  0.36569333, ...,  1.2547505 ,\n",
              "               -0.83006126,  0.7196638 ],\n",
              "              [-0.92075056,  1.079048  ,  0.53432435, ...,  1.25573   ,\n",
              "               -0.8130687 ,  0.746183  ],\n",
              "              ...,\n",
              "              [-0.8565994 ,  1.1005057 ,  0.48832598, ...,  1.2708396 ,\n",
              "               -0.8034706 ,  0.732224  ],\n",
              "              [-0.87043786,  1.0520198 ,  0.3964416 , ...,  1.2288551 ,\n",
              "               -0.8427248 ,  0.7607371 ],\n",
              "              [-0.79132366,  1.0826106 ,  0.3609904 , ...,  1.252331  ,\n",
              "               -0.82865536,  0.72807354]],\n",
              "\n",
              "             [[ 0.8339335 , -1.0510732 , -0.3113187 , ..., -1.2303994 ,\n",
              "                0.7995722 , -0.8335007 ],\n",
              "              [ 0.8298403 , -1.0592809 , -0.31267756, ..., -1.2349359 ,\n",
              "                0.7909504 , -0.8417046 ],\n",
              "              [ 0.85465586, -1.0718495 , -0.36612993, ..., -1.2443761 ,\n",
              "                0.77954763, -0.8440586 ],\n",
              "              ...,\n",
              "              [-0.62165374,  1.1764381 ,  0.06661539, ...,  1.185097  ,\n",
              "               -0.39629272,  1.0919832 ],\n",
              "              [ 0.90283155, -1.1054996 , -0.61631256, ..., -1.3063196 ,\n",
              "                0.864921  , -0.6820605 ],\n",
              "              [-0.61341006,  1.1802839 ,  0.06870057, ...,  1.1912264 ,\n",
              "               -0.41115323,  1.0747586 ]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.init(random.PRNGKey(0), example_x, example_y).keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6p52-tsCMsa",
        "outputId": "954d1336-9afb-4f9b-9de7-fb500c52f1f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "frozen_dict_keys(['params'])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training functions"
      ],
      "metadata": {
        "id": "1KgaDv1QJNWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from optax._src.base import Updates\n",
        "\n",
        "def predict_logits(params, input_sequences):\n",
        "  # TODO: The second input parameter should not be input_sequences\n",
        "  return transformer.apply({'params': params}, input_sequences, input_sequences)\n",
        "\n",
        "\n",
        "def loss_and_accuracy(params, input_sequences, targets):\n",
        "  \"\"\"Computes ce loss.\"\"\"\n",
        "  logits = predict_logits(params, input_sequences)\n",
        "  vocab_size = logits.shape[-1]\n",
        "  target_onehot = jax.nn.one_hot(n_targets, num_classes=vocab_size)\n",
        "  loss = optax.softmax_cross_entropy(logits, target_onehot).mean()\n",
        "  accuracy = (logits.argmax(axis=-1) == targets).astype(jnp.float32).mean()\n",
        "  return loss, accuracy\n",
        "\n",
        "@jax.jit\n",
        "def train_step(params, opt_state, batch):\n",
        "  _, targets = batch\n",
        "  input_sequences = batch_to_input(batch)\n",
        "  # Calculate loss value and its gradients by the value_and_grad function\n",
        "  loss_fn = lambda params: loss_and_accuracy(params, input_sequences, targets)\n",
        "  ret, grads = jax.value_and_grad(\n",
        "      loss_fn,\n",
        "      has_aux=True)(params)\n",
        "  loss, acc = ret[0], ret[1]\n",
        "  # Update the parameters\n",
        "  updates, opt_state = optimizer.update(grads, opt_state, params)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  return params, opt_state, loss, acc\n",
        "\n",
        "def batch_to_input(batch):\n",
        "  inp_data, _ = batch\n",
        "  # There are 10 digits (0, 1, ..., 9) and therefore when we make the \"embedding\",\n",
        "  # num_classes is 10.\n",
        "  inp_data = jax.nn.one_hot(inp_data, num_classes=10)\n",
        "  return inp_data\n",
        "\n",
        "def train_epoch(train_loader, epoch_idx: int, opt_state, params):\n",
        "  accs, losses = [], []\n",
        "  for batch in train_loader:\n",
        "    params, opt_state, loss, accuracy = train_step(params, opt_state, batch)\n",
        "    losses.append(loss)\n",
        "    accs.append(accuracy)\n",
        "  avg_loss = np.stack(jax.device_get(losses)).mean()\n",
        "  avg_acc = np.stack(jax.device_get(accs)).mean()\n",
        "\n",
        "def train_model(train_loader, val_loader, opt_state, params, num_epochs: int = 2):\n",
        "  # Train model for defined number of epochs\n",
        "  # best_acc = 0.0\n",
        "  for epoch_idx in range(1, num_epochs+1):\n",
        "    train_epoch(train_loader, epoch_idx=epoch_idx, opt_state=opt_state,\n",
        "                params=params)\n",
        "    # if epoch_idx % 5 == 0:\n",
        "    #   eval_acc = eval_model(val_loader)\n",
        "    #   logger.add_scalar('val/accuracy', eval_acc, global_step=epoch_idx)\n",
        "    #   if eval_acc >= best_acc:\n",
        "    #     best_acc = eval_acc\n",
        "    #     save_model(step=epoch_idx)\n",
        "    #   self.logger.flush()"
      ],
      "metadata": {
        "id": "uXA8469HB-lX",
        "outputId": "6d70f798-cb0e-4898-d928-f9cf9d095850",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9ab1936bfec7>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'jax' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing the dataset"
      ],
      "metadata": {
        "id": "IgeEccMNsD0b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset 1 Reversed Sequence"
      ],
      "metadata": {
        "id": "ls6UBvG4z-Ks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a map-style PyTorch dataset\n",
        "# (see more documentation at https://pytorch.org/docs/stable/data.html)\n",
        "class ReverseDataset(data.Dataset):\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_categories: int,\n",
        "                 seq_len: int,\n",
        "                 size: int,\n",
        "                 np_rng: ...):\n",
        "        super().__init__()\n",
        "        self.num_categories = num_categories\n",
        "        self.seq_len = seq_len\n",
        "        self.size = size\n",
        "        self.np_rng = np_rng\n",
        "\n",
        "        self.data = self.np_rng.integers(self.num_categories, size=(self.size, self.seq_len))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_data = self.data[idx]\n",
        "        labels = np.flip(input_data, axis=0)\n",
        "        return input_data, labels"
      ],
      "metadata": {
        "id": "XxyAITGp0E6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine batch elements (all numpy) by stacking\n",
        "def numpy_collate(batch):\n",
        "    if isinstance(batch[0], np.ndarray):\n",
        "        return np.stack(batch)\n",
        "    elif isinstance(batch[0], (tuple,list)):\n",
        "        transposed = zip(*batch)\n",
        "        return [numpy_collate(samples) for samples in transposed]\n",
        "    else:\n",
        "        return np.array(batch)\n",
        "\n",
        "dataset = partial(ReverseDataset, 20, 10)\n",
        "rev_train_loader = data.DataLoader(dataset(50000, np_rng=np.random.default_rng(42)),\n",
        "                                   batch_size=64,\n",
        "                                   shuffle=True,\n",
        "                                   drop_last=True,\n",
        "                                   collate_fn=numpy_collate)\n",
        "rev_val_loader   = data.DataLoader(dataset(1000, np_rng=np.random.default_rng(43)),\n",
        "                                   batch_size=64,\n",
        "                                   collate_fn=numpy_collate)\n",
        "rev_test_loader  = data.DataLoader(dataset(10000, np_rng=np.random.default_rng(44)),\n",
        "                                   batch_size=64,\n",
        "                                   collate_fn=numpy_collate)"
      ],
      "metadata": {
        "id": "pVBKoe0u0kq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp_data, labels = rev_train_loader.dataset[0]\n",
        "print(\"Input data:\", inp_data)\n",
        "print(\"Labels:    \", labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80NBJEFoBesV",
        "outputId": "cf3a3093-10a3-4990-f655-019078386043"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input data: [ 1 15 13  8  8 17  1 13  4  1]\n",
            "Labels:     [ 1  4 13  1 17  8  8 13 15  1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "YZClm5QxzvpG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training loop"
      ],
      "metadata": {
        "id": "1rSMArI8FrBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optax.adam(learning_rate=1e-2)"
      ],
      "metadata": {
        "id": "SsXcqtJz2m5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt_state = optimizer.init(params)"
      ],
      "metadata": {
        "id": "3FBhnZpeH1rE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(rev_train_loader, rev_val_loader, params=params, opt_state=opt_state)"
      ],
      "metadata": {
        "id": "ZxCJe5PCKfJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3N0ubHn_DjA9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}